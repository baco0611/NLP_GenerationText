{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('vinai/phobert-base')\n",
    "phobert_model = AutoModel.from_pretrained('vinai/phobert-base', config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./0.VanHoaFull/encoded_texts.npz')\n",
    "X = data['X']\n",
    "Y = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi dữ liệu thành Tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhoBertForTextGeneration(nn.Module):\n",
    "    def __init__(self, phobert_model):\n",
    "        super(PhoBertForTextGeneration, self).__init__()\n",
    "        self.phobert = phobert_model\n",
    "        self.config = phobert_model.config  # Lưu cấu hình từ mô hình gốc\n",
    "        self.linear = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.linear(sequence_output)\n",
    "        return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng GPU nếu có\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "# model = PhoBertForTextGeneration(phobert_model, config)\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# model = PhoBertForTextGeneration(phobert_model, vocab_size).to(device)\n",
    "model = PhoBertForTextGeneration(phobert_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm huấn luyện\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        # Chuyển dữ liệu sang device phù hợp\n",
    "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # Xóa gradient cũ\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Tính toán đầu ra của mô hình\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        # Shape của outputs là [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # Tính toán độ lỗi\n",
    "        loss = CrossEntropyLoss()(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Trả về độ lỗi trung bình\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \tLoss: 9.08\n",
      "Epoch 2 \tLoss: 6.29\n",
      "Epoch 3 \tLoss: 4.92\n",
      "Epoch 4 \tLoss: 3.94\n",
      "Epoch 5 \tLoss: 3.20\n",
      "Epoch 6 \tLoss: 2.64\n",
      "Epoch 7 \tLoss: 2.23\n",
      "Epoch 8 \tLoss: 1.91\n",
      "Epoch 9 \tLoss: 1.66\n",
      "Epoch 10 \tLoss: 1.45\n",
      "Epoch 11 \tLoss: 1.28\n",
      "Epoch 12 \tLoss: 1.14\n",
      "Epoch 13 \tLoss: 1.02\n",
      "Epoch 14 \tLoss: 0.92\n",
      "Epoch 15 \tLoss: 0.83\n",
      "Epoch 16 \tLoss: 0.76\n",
      "Epoch 17 \tLoss: 0.69\n",
      "Epoch 18 \tLoss: 0.63\n",
      "Epoch 19 \tLoss: 0.57\n",
      "Epoch 20 \tLoss: 0.52\n",
      "Epoch 21 \tLoss: 0.48\n",
      "Epoch 22 \tLoss: 0.44\n",
      "Epoch 23 \tLoss: 0.40\n",
      "Epoch 24 \tLoss: 0.37\n",
      "Epoch 25 \tLoss: 0.34\n",
      "Epoch 26 \tLoss: 0.31\n",
      "Epoch 27 \tLoss: 0.28\n",
      "Epoch 28 \tLoss: 0.26\n",
      "Epoch 29 \tLoss: 0.24\n",
      "Epoch 30 \tLoss: 0.22\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} \", end=\"\\t\")\n",
    "    avg_loss = train(model, train_loader, optimizer)\n",
    "    print(f\"Loss: {avg_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'phobert_text_generation_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhoBertForTextGeneration(phobert_model)\n",
    "\n",
    "# Tải trạng thái mô hình\n",
    "model.load_state_dict(torch.load('phobert_text_generation_model.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(input_ids=input_ids)\n",
    "            loss = CrossEntropyLoss()(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.5260931600534433\n"
     ]
    }
   ],
   "source": [
    "# Tính độ lỗi trung bình trên tập kiểm thử\n",
    "avg_test_loss = evaluate(model, test_loader)\n",
    "perplexity = m.exp(avg_test_loss)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm huấn luyện\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        loss = CrossEntropyLoss()(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
